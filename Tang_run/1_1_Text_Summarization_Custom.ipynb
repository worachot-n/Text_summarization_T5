{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/worachot-n/Text_summarization_T5/blob/main/1_1_Text_Summarization_Custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0J9IBaBMfm1",
        "outputId": "abca27f3-a340-4808-9232-7fc2f7d45e35"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRTA4OL4MpCP",
        "outputId": "acb0b018-9d8e-4cb0-a8f4-f62f3c8e4549"
      },
      "outputs": [],
      "source": [
        "# !pip install sentencepiece\n",
        "# !pip install transformers\n",
        "# !pip install rich[jupyter]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MICsXjGZMpiQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"BBC_News_Summary.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "da8IYf2nMtqS",
        "outputId": "fc5fbf8a-a870-4136-c091-4ffe334e278b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>articles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>414</td>\n",
              "      <td>Blair rejects Iraq advice calls\\r\\n\\r\\nTony Bl...</td>\n",
              "      <td>Ms Hewitt also announced a new drive to help w...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>420</td>\n",
              "      <td>Strachan turns down Pompey\\r\\n\\r\\nFormer South...</td>\n",
              "      <td>Fifth seed Robin Soderling took the Milan Indo...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>1644</td>\n",
              "      <td>Downloads enter US singles chart\\r\\n\\r\\nDigita...</td>\n",
              "      <td>Bridget Jones: The Edge of Reason was named Ev...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>416</td>\n",
              "      <td>Chancellor rallies Labour voters\\r\\n\\r\\nGordon...</td>\n",
              "      <td>Mr Blair said that whether the public chose Mi...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>1232</td>\n",
              "      <td>Sony wares win innovation award\\r\\n\\r\\nSony ha...</td>\n",
              "      <td>Mr Raskin was one of the first employees at Ap...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1544</th>\n",
              "      <td>1544</td>\n",
              "      <td>Little Britain two top comic list\\r\\n\\r\\nLittl...</td>\n",
              "      <td>It is predicted that the Band Aid 20 song will...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1748</th>\n",
              "      <td>1748</td>\n",
              "      <td>Market unfazed by Aurora setback\\r\\n\\r\\nAs the...</td>\n",
              "      <td>Shares in US phone company MCI have risen on s...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>1264</td>\n",
              "      <td>Ask Jeeves joins web log market\\r\\n\\r\\nAsk Jee...</td>\n",
              "      <td>A US woman is suing Hewlett Packard (HP), sayi...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>629</th>\n",
              "      <td>629</td>\n",
              "      <td>Yelling takes Cardiff hat-trick\\r\\n\\r\\nEuropea...</td>\n",
              "      <td>Day has been displaced by the arrival of Simon...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043</th>\n",
              "      <td>1043</td>\n",
              "      <td>Cyber crime booms in 2004\\r\\n\\r\\nThe last 12 m...</td>\n",
              "      <td>The Gameboyzz Orchestra Project has taken the ...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0                                           articles  \\\n",
              "414          414  Blair rejects Iraq advice calls\\r\\n\\r\\nTony Bl...   \n",
              "420          420  Strachan turns down Pompey\\r\\n\\r\\nFormer South...   \n",
              "1644        1644  Downloads enter US singles chart\\r\\n\\r\\nDigita...   \n",
              "416          416  Chancellor rallies Labour voters\\r\\n\\r\\nGordon...   \n",
              "1232        1232  Sony wares win innovation award\\r\\n\\r\\nSony ha...   \n",
              "1544        1544  Little Britain two top comic list\\r\\n\\r\\nLittl...   \n",
              "1748        1748  Market unfazed by Aurora setback\\r\\n\\r\\nAs the...   \n",
              "1264        1264  Ask Jeeves joins web log market\\r\\n\\r\\nAsk Jee...   \n",
              "629          629  Yelling takes Cardiff hat-trick\\r\\n\\r\\nEuropea...   \n",
              "1043        1043  Cyber crime booms in 2004\\r\\n\\r\\nThe last 12 m...   \n",
              "\n",
              "                                              summaries     categories  \n",
              "414   Ms Hewitt also announced a new drive to help w...       politics  \n",
              "420   Fifth seed Robin Soderling took the Milan Indo...          sport  \n",
              "1644  Bridget Jones: The Edge of Reason was named Ev...  entertainment  \n",
              "416   Mr Blair said that whether the public chose Mi...       politics  \n",
              "1232  Mr Raskin was one of the first employees at Ap...           tech  \n",
              "1544  It is predicted that the Band Aid 20 song will...  entertainment  \n",
              "1748  Shares in US phone company MCI have risen on s...       business  \n",
              "1264  A US woman is suing Hewlett Packard (HP), sayi...           tech  \n",
              "629   Day has been displaced by the arrival of Simon...          sport  \n",
              "1043  The Gameboyzz Orchestra Project has taken the ...           tech  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tcz7rxMrjbF7"
      },
      "outputs": [],
      "source": [
        "df_business = df.loc[df['categories'] == 'business']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1kBUyxewjlSt",
        "outputId": "a239785b-e703-44b7-b276-2395a042141f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articles</th>\n",
              "      <th>summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>EU 'too slow' on economic reforms\\r\\n\\r\\nMost ...</td>\n",
              "      <td>Senator Kerry attacked President Bush's econom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>Glazer makes new Man Utd approach\\r\\n\\r\\nMalco...</td>\n",
              "      <td>A slimming aid made from a southern African ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>Five million Germans out of work\\r\\n\\r\\nGerman...</td>\n",
              "      <td>Job creation was one of last year's main conce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1718</th>\n",
              "      <td>Axa Sun Life cuts bonus payments\\r\\n\\r\\nLife i...</td>\n",
              "      <td>Malcolm Glazer has made a fresh approach to bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>Amex shares up on spin-off news\\r\\n\\r\\nShares ...</td>\n",
              "      <td>If admitted into the EU, Turkey would contribu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>GE sees 'excellent' world economy\\r\\n\\r\\nUS be...</td>\n",
              "      <td>The WTO said that many developing countries su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>Millions 'to lose textile jobs'\\r\\n\\r\\nMillion...</td>\n",
              "      <td>Reuters news agency reported that Iraq's Oil M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>US to rule on Yukos refuge call\\r\\n\\r\\nYukos h...</td>\n",
              "      <td>Parmalat has sued 45 banks as it tries to recl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>Business confidence dips in Japan\\r\\n\\r\\nBusin...</td>\n",
              "      <td>Bombardier said restructuring plans drawn up b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>Wall Street cheers Bush victory\\r\\n\\r\\nThe US ...</td>\n",
              "      <td>The Brazilian government has played down claim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>510 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               articles  \\\n",
              "1715  EU 'too slow' on economic reforms\\r\\n\\r\\nMost ...   \n",
              "1716  Glazer makes new Man Utd approach\\r\\n\\r\\nMalco...   \n",
              "1717  Five million Germans out of work\\r\\n\\r\\nGerman...   \n",
              "1718  Axa Sun Life cuts bonus payments\\r\\n\\r\\nLife i...   \n",
              "1719  Amex shares up on spin-off news\\r\\n\\r\\nShares ...   \n",
              "...                                                 ...   \n",
              "2220  GE sees 'excellent' world economy\\r\\n\\r\\nUS be...   \n",
              "2221  Millions 'to lose textile jobs'\\r\\n\\r\\nMillion...   \n",
              "2222  US to rule on Yukos refuge call\\r\\n\\r\\nYukos h...   \n",
              "2223  Business confidence dips in Japan\\r\\n\\r\\nBusin...   \n",
              "2224  Wall Street cheers Bush victory\\r\\n\\r\\nThe US ...   \n",
              "\n",
              "                                              summaries  \n",
              "1715  Senator Kerry attacked President Bush's econom...  \n",
              "1716  A slimming aid made from a southern African ca...  \n",
              "1717  Job creation was one of last year's main conce...  \n",
              "1718  Malcolm Glazer has made a fresh approach to bu...  \n",
              "1719  If admitted into the EU, Turkey would contribu...  \n",
              "...                                                 ...  \n",
              "2220  The WTO said that many developing countries su...  \n",
              "2221  Reuters news agency reported that Iraq's Oil M...  \n",
              "2222  Parmalat has sued 45 banks as it tries to recl...  \n",
              "2223  Bombardier said restructuring plans drawn up b...  \n",
              "2224  The Brazilian government has played down claim...  \n",
              "\n",
              "[510 rows x 2 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_business = df_business[['articles', 'summaries']]\n",
        "df_business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "TujDzy_Lkyn8",
        "outputId": "e6ae5bf4-3855-4799-da46-a52763401c85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articles</th>\n",
              "      <th>summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>EU 'too slow' on economic reforms\\r\\n\\r\\nMost ...</td>\n",
              "      <td>Senator Kerry attacked President Bush's econom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>Glazer makes new Man Utd approach\\r\\n\\r\\nMalco...</td>\n",
              "      <td>A slimming aid made from a southern African ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>Five million Germans out of work\\r\\n\\r\\nGerman...</td>\n",
              "      <td>Job creation was one of last year's main conce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1718</th>\n",
              "      <td>Axa Sun Life cuts bonus payments\\r\\n\\r\\nLife i...</td>\n",
              "      <td>Malcolm Glazer has made a fresh approach to bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>Amex shares up on spin-off news\\r\\n\\r\\nShares ...</td>\n",
              "      <td>If admitted into the EU, Turkey would contribu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>GE sees 'excellent' world economy\\r\\n\\r\\nUS be...</td>\n",
              "      <td>The WTO said that many developing countries su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2221</th>\n",
              "      <td>Millions 'to lose textile jobs'\\r\\n\\r\\nMillion...</td>\n",
              "      <td>Reuters news agency reported that Iraq's Oil M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2222</th>\n",
              "      <td>US to rule on Yukos refuge call\\r\\n\\r\\nYukos h...</td>\n",
              "      <td>Parmalat has sued 45 banks as it tries to recl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2223</th>\n",
              "      <td>Business confidence dips in Japan\\r\\n\\r\\nBusin...</td>\n",
              "      <td>Bombardier said restructuring plans drawn up b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2224</th>\n",
              "      <td>Wall Street cheers Bush victory\\r\\n\\r\\nThe US ...</td>\n",
              "      <td>The Brazilian government has played down claim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>510 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               articles  \\\n",
              "1715  EU 'too slow' on economic reforms\\r\\n\\r\\nMost ...   \n",
              "1716  Glazer makes new Man Utd approach\\r\\n\\r\\nMalco...   \n",
              "1717  Five million Germans out of work\\r\\n\\r\\nGerman...   \n",
              "1718  Axa Sun Life cuts bonus payments\\r\\n\\r\\nLife i...   \n",
              "1719  Amex shares up on spin-off news\\r\\n\\r\\nShares ...   \n",
              "...                                                 ...   \n",
              "2220  GE sees 'excellent' world economy\\r\\n\\r\\nUS be...   \n",
              "2221  Millions 'to lose textile jobs'\\r\\n\\r\\nMillion...   \n",
              "2222  US to rule on Yukos refuge call\\r\\n\\r\\nYukos h...   \n",
              "2223  Business confidence dips in Japan\\r\\n\\r\\nBusin...   \n",
              "2224  Wall Street cheers Bush victory\\r\\n\\r\\nThe US ...   \n",
              "\n",
              "                                              summaries  \n",
              "1715  Senator Kerry attacked President Bush's econom...  \n",
              "1716  A slimming aid made from a southern African ca...  \n",
              "1717  Job creation was one of last year's main conce...  \n",
              "1718  Malcolm Glazer has made a fresh approach to bu...  \n",
              "1719  If admitted into the EU, Turkey would contribu...  \n",
              "...                                                 ...  \n",
              "2220  The WTO said that many developing countries su...  \n",
              "2221  Reuters news agency reported that Iraq's Oil M...  \n",
              "2222  Parmalat has sued 45 banks as it tries to recl...  \n",
              "2223  Bombardier said restructuring plans drawn up b...  \n",
              "2224  The Brazilian government has played down claim...  \n",
              "\n",
              "[510 rows x 2 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_business = df_business.replace(r'\\n\\n',' ', regex=True)\n",
        "df_business"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6RPLMwliM06B"
      },
      "outputs": [],
      "source": [
        "df_business[\"summaries\"] = \"summarize: \"+df_business[\"summaries\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "G6uMPmOTM26G",
        "outputId": "f99d9043-29d5-4430-d9b0-46deb0dec865"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articles</th>\n",
              "      <th>summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1715</th>\n",
              "      <td>EU 'too slow' on economic reforms\\r\\n\\r\\nMost ...</td>\n",
              "      <td>summarize: Senator Kerry attacked President Bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1716</th>\n",
              "      <td>Glazer makes new Man Utd approach\\r\\n\\r\\nMalco...</td>\n",
              "      <td>summarize: A slimming aid made from a southern...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1717</th>\n",
              "      <td>Five million Germans out of work\\r\\n\\r\\nGerman...</td>\n",
              "      <td>summarize: Job creation was one of last year's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1718</th>\n",
              "      <td>Axa Sun Life cuts bonus payments\\r\\n\\r\\nLife i...</td>\n",
              "      <td>summarize: Malcolm Glazer has made a fresh app...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1719</th>\n",
              "      <td>Amex shares up on spin-off news\\r\\n\\r\\nShares ...</td>\n",
              "      <td>summarize: If admitted into the EU, Turkey wou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               articles  \\\n",
              "1715  EU 'too slow' on economic reforms\\r\\n\\r\\nMost ...   \n",
              "1716  Glazer makes new Man Utd approach\\r\\n\\r\\nMalco...   \n",
              "1717  Five million Germans out of work\\r\\n\\r\\nGerman...   \n",
              "1718  Axa Sun Life cuts bonus payments\\r\\n\\r\\nLife i...   \n",
              "1719  Amex shares up on spin-off news\\r\\n\\r\\nShares ...   \n",
              "\n",
              "                                              summaries  \n",
              "1715  summarize: Senator Kerry attacked President Bu...  \n",
              "1716  summarize: A slimming aid made from a southern...  \n",
              "1717  summarize: Job creation was one of last year's...  \n",
              "1718  summarize: Malcolm Glazer has made a fresh app...  \n",
              "1719  summarize: If admitted into the EU, Turkey wou...  "
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_business.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "G9UtBc8tM7U_"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "from rich.table import Column, Table\n",
        "from rich import box\n",
        "from rich.console import Console\n",
        "\n",
        "# define a rich console logger\n",
        "console=Console(record=True)\n",
        "\n",
        "def display_df(df):\n",
        "  \"\"\"display dataframe in ASCII format\"\"\"\n",
        "\n",
        "  console=Console()\n",
        "  table = Table(Column(\"source_text\", justify=\"center\" ), Column(\"target_text\", justify=\"center\"), title=\"Sample Data\",pad_edge=False, box=box.ASCII)\n",
        "\n",
        "  for i, row in enumerate(df.values.tolist()):\n",
        "    table.add_row(row[0], row[1])\n",
        "\n",
        "  console.print(table)\n",
        "\n",
        "training_logger = Table(Column(\"Epoch\", justify=\"center\" ), \n",
        "                        Column(\"Steps\", justify=\"center\"),\n",
        "                        Column(\"Loss\", justify=\"center\"), \n",
        "                        title=\"Training Status\",pad_edge=False, box=box.ASCII)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GhVlUbrfM9Ld"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kS4zDtn-NBzg"
      },
      "outputs": [],
      "source": [
        "class YourDataSetClass(Dataset):\n",
        "  \"\"\"\n",
        "  Creating a custom dataset for reading the dataset and \n",
        "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = dataframe\n",
        "    self.source_len = source_len\n",
        "    self.summ_len = target_len\n",
        "    self.target_text = self.data[target_text]\n",
        "    self.source_text = self.data[source_text]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target_text)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    source_text = str(self.source_text[index])\n",
        "    target_text = str(self.target_text[index])\n",
        "\n",
        "    #cleaning data so as to ensure data is in string type\n",
        "    source_text = ' '.join(source_text.split())\n",
        "    target_text = ' '.join(target_text.split())\n",
        "\n",
        "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "\n",
        "    source_ids = source['input_ids'].squeeze()\n",
        "    source_mask = source['attention_mask'].squeeze()\n",
        "    target_ids = target['input_ids'].squeeze()\n",
        "    target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "    return {\n",
        "        'source_ids': source_ids.to(dtype=torch.long), \n",
        "        'source_mask': source_mask.to(dtype=torch.long), \n",
        "        'target_ids': target_ids.to(dtype=torch.long),\n",
        "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "s-hI5IsVNG3y"
      },
      "outputs": [],
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to be called for training with the parameters passed from main function\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model.train()\n",
        "  for _,data in enumerate(loader, 0):\n",
        "    y = data['target_ids'].to(device, dtype = torch.long)\n",
        "    y_ids = y[:, :-1].contiguous()\n",
        "    lm_labels = y[:, 1:].clone().detach()\n",
        "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    if _%10==0:\n",
        "      training_logger.add_row(str(epoch), str(_), str(loss))\n",
        "      console.print(training_logger)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "dJ9KvreONHaO"
      },
      "outputs": [],
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "\n",
        "  \"\"\"\n",
        "  Function to evaluate model for predictions\n",
        "\n",
        "  \"\"\"\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "      for _, data in enumerate(loader, 0):\n",
        "          y = data['target_ids'].to(device, dtype = torch.long)\n",
        "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "          generated_ids = model.generate(\n",
        "              input_ids = ids,\n",
        "              attention_mask = mask, \n",
        "              max_length=150, \n",
        "              num_beams=2,\n",
        "              repetition_penalty=2.5, \n",
        "              length_penalty=1.0, \n",
        "              early_stopping=True\n",
        "              )\n",
        "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "          if _%10==0:\n",
        "              console.print(f'Completed {_}')\n",
        "\n",
        "          predictions.extend(preds)\n",
        "          actuals.extend(target)\n",
        "  return predictions, actuals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "M9Mepxs7NJTh"
      },
      "outputs": [],
      "source": [
        "def T5Trainer(dataframe, source_text, target_text, model_params, output_dir=\"./outputs/\" ):\n",
        "  \n",
        "  \"\"\"\n",
        "  T5 trainer\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Set random seeds and deterministic pytorch for reproducibility\n",
        "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
        "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  # logging\n",
        "  console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
        "\n",
        "  # tokenzier for encoding the text\n",
        "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
        "\n",
        "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
        "  model = model.to(device)\n",
        "  \n",
        "  # logging\n",
        "  console.log(f\"[Data]: Reading data...\\n\")\n",
        "\n",
        "  # Importing the raw dataset\n",
        "  dataframe = dataframe[[source_text,target_text]]\n",
        "  display_df(dataframe.head(2))\n",
        "\n",
        "  \n",
        "  # Creation of Dataset and Dataloader\n",
        "  # Defining the train size. So 80% of the data will be used for training and the rest for validation. \n",
        "  train_size = 0.8\n",
        "  train_dataset=dataframe.sample(frac=train_size,random_state = model_params[\"SEED\"])\n",
        "  val_dataset=dataframe.drop(train_dataset.index).reset_index(drop=True)\n",
        "  train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "  console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
        "  console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
        "  console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
        "\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "  training_set = YourDataSetClass(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "  val_set = YourDataSetClass(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
        "\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  train_params = {\n",
        "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0\n",
        "      }\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "  training_loader = DataLoader(training_set, **train_params)\n",
        "  val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=model_params[\"LEARNING_RATE\"])\n",
        "\n",
        "\n",
        "  # Training loop\n",
        "  console.log(f'[Initiating Fine Tuning]...\\n')\n",
        "\n",
        "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
        "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "      \n",
        "  console.log(f\"[Saving Model]...\\n\")\n",
        "  #Saving the model after training\n",
        "  path = os.path.join(output_dir, \"model_files\")\n",
        "  model.save_pretrained(path)\n",
        "  tokenizer.save_pretrained(path)\n",
        "\n",
        "\n",
        "  # evaluating test dataset\n",
        "  console.log(f\"[Initiating Validation]...\\n\")\n",
        "  for epoch in range(model_params[\"VAL_EPOCHS\"]):\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv(os.path.join(output_dir,'predictions.csv'))\n",
        "  \n",
        "  console.save_text(os.path.join(output_dir,'logs.txt'))\n",
        "  \n",
        "  console.log(f\"[Validation Completed.]\\n\")\n",
        "  console.print(f\"\"\"[Model] Model saved @ {os.path.join(output_dir, \"model_files\")}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Validation] Generation on Validation data saved @ {os.path.join(output_dir,'predictions.csv')}\\n\"\"\")\n",
        "  console.print(f\"\"\"[Logs] Logs saved @ {os.path.join(output_dir,'logs.txt')}\\n\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "B9tMihWuNMBG"
      },
      "outputs": [],
      "source": [
        "model_params={\n",
        "    \"MODEL\":\"t5-base\",             # model_type: t5-base/t5-large\n",
        "    \"TRAIN_BATCH_SIZE\":4,          # training batch size\n",
        "    \"VALID_BATCH_SIZE\":4,          # validation batch size\n",
        "    \"TRAIN_EPOCHS\":3,              # number of training epochs\n",
        "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
        "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
        "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
        "    \"MAX_TARGET_TEXT_LENGTH\":50,   # max length of target text\n",
        "    \"SEED\": 42                     # set seed for reproducibility \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FlXkJrArNOCP",
        "outputId": "5adde02c-bdf3-4c3a-9002-da023526b369"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[18:00:46] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-base<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-33-b1eef7f6be59&gt;:14</span>\n",
              "                                                                                             \n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[18:00:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-base\u001b[33m...\u001b[0m                     \u001b[2m<ipython-input-33-b1eef7f6be59>\u001b[0m\u001b[2m:\u001b[0m\u001b[2m14\u001b[0m\n",
              "                                                                                             \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 6.00 GiB total capacity; 4.42 GiB already allocated; 0 bytes free; 4.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-35-35faf8faf7d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mT5Trainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_business\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"articles\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"summaries\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-33-b1eef7f6be59>\u001b[0m in \u001b[0;36mT5Trainer\u001b[1;34m(dataframe, source_text, target_text, model_params, output_dir)\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;31m# Further this model is sent to device (GPU/TPU) for using the hardware.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"MODEL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[1;31m# logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 6.00 GiB total capacity; 4.42 GiB already allocated; 0 bytes free; 4.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "T5Trainer(dataframe=df_business[:500], source_text=\"articles\", target_text=\"summaries\", model_params=model_params, output_dir=\"outputs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM1g2XZVSTC1zcFW5mlKOht",
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "1.1 Text_Summarization_Custom",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
